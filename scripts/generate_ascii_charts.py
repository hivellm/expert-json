#!/usr/bin/env python3
"""
Generate ASCII/text charts for dataset documentation (no external dependencies)
"""
import json
from pathlib import Path

def create_bar_chart(data, max_width=50, label_width=25):
    """Create ASCII horizontal bar chart"""
    if not data:
        return ""
    
    max_value = max(data.values())
    lines = []
    
    for label, value in sorted(data.items(), key=lambda x: -x[1]):
        bar_width = int((value / max_value) * max_width)
        bar = 'â–ˆ' * bar_width
        percentage = (value / sum(data.values()) * 100)
        line = f"{label:<{label_width}} {bar} {value:>8,} ({percentage:>5.1f}%)"
        lines.append(line)
    
    return '\n'.join(lines)

def create_distribution_chart(values_dict, title, max_width=60):
    """Create distribution visualization"""
    total = sum(values_dict.values())
    
    lines = [f"\n{title}", "=" * max_width]
    
    for label, value in sorted(values_dict.items(), key=lambda x: -x[1]):
        pct = value / total * 100
        bar_len = int((value / total) * max_width)
        bar = 'â–“' * bar_len
        lines.append(f"{label:20s} [{bar:<{max_width}}] {value:>6,} ({pct:>5.1f}%)")
    
    lines.append("=" * max_width)
    lines.append(f"{'TOTAL':20s} {' ' * max_width} {total:>6,} (100.0%)")
    
    return '\n'.join(lines)

def generate_markdown_report(analysis_path, output_path):
    """Generate comprehensive markdown report with charts"""
    
    with open(analysis_path, 'r') as f:
        report = json.load(f)
    
    md = []
    
    # Header
    md.append("# Expert-JSON Dataset Analysis")
    md.append("")
    md.append("**Generated:** `dataset_analysis.json`  ")
    md.append(f"**Total Examples:** {report['summary']['total_examples']:,}  ")
    md.append(f"**Total Size:** {report['summary']['total_size_bytes'] / 1024 / 1024:.2f} MB  ")
    md.append("")
    
    # Overview
    md.append("## ðŸ“Š Overview")
    md.append("")
    md.append("| Metric | Value |")
    md.append("|--------|-------|")
    md.append(f"| Total Examples | {report['summary']['total_examples']:,} |")
    md.append(f"| Total Size | {report['summary']['total_size_bytes'] / 1024 / 1024:.2f} MB |")
    md.append(f"| Average Size | {report['summary']['total_size_bytes'] / report['summary']['total_examples']:.0f} chars |")
    md.append(f"| Median Size | {report['size_statistics']['median']:,} chars |")
    md.append("")
    
    # Task Distribution
    md.append("## ðŸŽ¯ Task Distribution")
    md.append("")
    md.append("```")
    task_chart = create_distribution_chart(
        report['task_distribution'],
        "Task Types",
        max_width=50
    )
    md.append(task_chart)
    md.append("```")
    md.append("")
    
    md.append("### Task Breakdown")
    md.append("")
    md.append("| Task Type | Count | Percentage |")
    md.append("|-----------|-------|------------|")
    for task, count in sorted(report['task_distribution'].items(), key=lambda x: -x[1]):
        pct = report['task_percentages'][task]
        md.append(f"| **{task.replace('_', ' ').title()}** | {count:,} | {pct:.1f}% |")
    md.append("")
    
    # Format Distribution
    md.append("## ðŸ“‹ Format Distribution")
    md.append("")
    md.append("```")
    format_chart = create_bar_chart(report['format_distribution'], max_width=40, label_width=25)
    md.append(format_chart)
    md.append("```")
    md.append("")
    
    md.append("### Format Breakdown")
    md.append("")
    md.append("| Format | Count | Percentage | Description |")
    md.append("|--------|-------|------------|-------------|")
    
    format_descriptions = {
        'data_extraction': 'Complex structured data extraction from documents',
        'cloudevents': 'CNCF CloudEvents specification format',
        'openapi_property': 'Individual OpenAPI property examples',
        'json_schema': 'JSON Schema compliant objects',
        'openapi_schema': 'Complete OpenAPI schema definitions',
        'openapi_response': 'API response body examples',
        'openapi_request': 'API request body examples'
    }
    
    for fmt, count in sorted(report['format_distribution'].items(), key=lambda x: -x[1]):
        pct = report['format_percentages'][fmt]
        desc = format_descriptions.get(fmt, '-')
        md.append(f"| `{fmt}` | {count:,} | {pct:.1f}% | {desc} |")
    md.append("")
    
    # Error Types
    if report.get('error_types'):
        md.append("## ðŸ”§ Error Types (Correction Tasks)")
        md.append("")
        md.append("```")
        error_chart = create_bar_chart(report['error_types'], max_width=30, label_width=20)
        md.append(error_chart)
        md.append("```")
        md.append("")
        
        md.append("### Error Type Details")
        md.append("")
        md.append("| Error Type | Count | Description |")
        md.append("|------------|-------|-------------|")
        
        error_descriptions = {
            'extra_field': 'Unknown/unexpected fields in JSON',
            'type_mismatch': 'Value type doesn\'t match schema',
            'syntax_error': 'Invalid JSON syntax (commas, quotes, etc)',
            'missing_field': 'Required field is absent'
        }
        
        for error, count in sorted(report['error_types'].items(), key=lambda x: -x[1]):
            desc = error_descriptions.get(error, '-')
            md.append(f"| `{error}` | {count:,} | {desc} |")
        md.append("")
    
    # Size Statistics
    md.append("## ðŸ“ Size Statistics")
    md.append("")
    
    size_stats = report['size_statistics']
    
    md.append("### Example Size Distribution (characters)")
    md.append("")
    md.append("| Percentile | Size | Visualization |")
    md.append("|------------|------|---------------|")
    
    max_size = size_stats['max']
    stats_order = [
        ('Min', size_stats['min']),
        ('P25', size_stats['p25']),
        ('Median', size_stats['median']),
        ('Mean', size_stats['mean']),
        ('P75', size_stats['p75']),
        ('P90', size_stats['p90']),
        ('P95', size_stats['p95']),
        ('Max', size_stats['max'])
    ]
    
    for label, value in stats_order:
        bar_len = min(50, int((value / max_size) * 50)) if max_size > 0 else 0
        bar = 'â–“' * bar_len
        md.append(f"| **{label}** | {value:>8,.0f} | `{bar}` |")
    md.append("")
    
    # JSON Output Size
    if report.get('json_size_statistics'):
        json_stats = report['json_size_statistics']
        md.append("### JSON Output Size (characters)")
        md.append("")
        md.append("| Metric | Size |")
        md.append("|--------|------|")
        md.append(f"| Min | {json_stats['min']:,} |")
        md.append(f"| Median | {json_stats['median']:,} |")
        md.append(f"| Mean | {json_stats['mean']:,.0f} |")
        md.append(f"| Max | {json_stats['max']:,} |")
        md.append("")
    
    # Schema Presence
    md.append("## ðŸ” Schema Presence")
    md.append("")
    
    schema = report['schema_presence']
    total_schema = schema['with_schema'] + schema['without_schema']
    with_pct = schema['with_schema'] / total_schema * 100 if total_schema > 0 else 0
    without_pct = 100 - with_pct
    
    md.append("| Type | Count | Percentage | Bar |")
    md.append("|------|-------|------------|-----|")
    
    with_bar = 'â–ˆ' * int(with_pct / 2)
    without_bar = 'â–ˆ' * int(without_pct / 2)
    
    md.append(f"| With Schema | {schema['with_schema']:,} | {with_pct:.1f}% | `{with_bar}` |")
    md.append(f"| Without Schema | {schema['without_schema']:,} | {without_pct:.1f}% | `{without_bar}` |")
    md.append("")
    
    # Quality Metrics
    md.append("## âœ… Quality Metrics")
    md.append("")
    md.append("| Metric | Value | Status |")
    md.append("|--------|-------|--------|")
    md.append(f"| JSON Validity | 100% | âœ… All examples valid |")
    md.append(f"| ChatML Format | 100% | âœ… All properly formatted |")
    md.append(f"| Deduplication | Applied | âœ… SHA-256 hashing |")
    md.append(f"| Size Filtering | Applied | âœ… Min 50 chars |")
    md.append(f"| Task Balance | {abs(report['task_percentages'].get('generation', 0) - 33.3):.1f}% deviation | {'âœ…' if abs(report['task_percentages'].get('generation', 0) - 33.3) < 10 else 'âš ï¸'} From 33/33/33 target |")
    md.append("")
    
    # Data Sources
    md.append("## ðŸ“š Data Sources")
    md.append("")
    md.append("| Source | Type | Examples | Quality |")
    md.append("|--------|------|----------|---------|")
    md.append("| **APIs.guru** | OpenAPI specifications | Real-world API schemas | â­â­â­â­ |")
    md.append("| **SchemaStore** | JSON Schemas | Configuration files | â­â­â­â­â­ |")
    md.append("| **CloudEvents** | Event format | CNCF standard events | â­â­â­â­â­ |")
    md.append("| **Paraloq** | Textâ†’JSON extraction | Medical, business docs | â­â­â­â­â­ |")
    md.append("| **MasterControl** | Document extraction | Complex structured data | â­â­â­â­â­ |")
    md.append("| **Generated Negatives** | Invalid JSON | Error correction training | â­â­â­â­ |")
    md.append("")
    
    # Use Cases
    md.append("## ðŸŽ¯ Supported Use Cases")
    md.append("")
    md.append("1. **JSON Generation** (60.5%)")
    md.append("   - OpenAPI request/response generation")
    md.append("   - JSON Schema compliant object creation")
    md.append("   - CloudEvents event formatting")
    md.append("   - Configuration file generation")
    md.append("")
    md.append("2. **JSON Correction** (39.5%)")
    md.append("   - Fix missing required fields")
    md.append("   - Correct type mismatches")
    md.append("   - Repair syntax errors")
    md.append("   - Remove extra/unknown fields")
    md.append("")
    
    # Recommendations
    md.append("## ðŸ’¡ Training Recommendations")
    md.append("")
    md.append("- **Batch Size:** 2 (memory constrained)")
    md.append("- **Gradient Accumulation:** 45 (effective batch: 90)")
    md.append("- **Learning Rate:** 5e-5")
    md.append("- **Epochs:** 1.5")
    md.append("- **Adapter:** DoRA r=14")
    md.append("- **Temperature:** 0.7 (inference)")
    md.append("- **Grammar:** JSON GBNF enabled")
    md.append("")
    
    # Write file
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(md))
    
    print(f"âœ“ Generated: {output_path}")

def main():
    analysis_path = Path('docs/dataset_analysis.json')
    output_path = Path('docs/DATASET_DISTRIBUTION.md')
    
    if not analysis_path.exists():
        print(f"Error: {analysis_path} not found")
        print("Run: python3 scripts/analyze_dataset.py")
        return 1
    
    print("Generating markdown documentation with charts...")
    generate_markdown_report(analysis_path, output_path)
    print("\nâœ“ Documentation generated successfully!")
    
    return 0

if __name__ == '__main__':
    exit(main())

